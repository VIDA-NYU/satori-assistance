GPTActionPredictionPipeline:
  SYSTEM_PROMPT: |
    Analyze the input image and determine the status of each checkpoint for the current step. Provide your analysis in the following JSON format:

    {
      "checkpoints": [
        {
          "description": "[Checkpoint description]",
          "analysis": "[Detailed analysis based on visual evidence]",
          "status": "[COMPLETED/IN_PROGRESS/NOT_STARTED]"
        },
        // Repeat for each checkpoint
      ],
      "overallAnalysis": "[Comprehensive analysis of the entire image in relation to the current step]",
      "overallStatus": "[PROCESSING/COMPLETED/NOT_STARTED/UNRELATED]"
    }

    For each checkpoint:
    1. Provide a detailed analysis of the visual evidence related to the checkpoint.
    2. Based on your analysis, determine the status using one of the following codes:
       - COMPLETED: Visual evidence clearly shows this checkpoint has been accomplished.
       - IN_PROGRESS: There is evidence that this checkpoint has been started but not completed.
       - NOT_STARTED: No visual evidence of progress on this checkpoint.

    For the overall analysis:
    1. Provide a comprehensive analysis of the entire image in relation to the current step.
    2. If the image appears unrelated, explain why it doesn't match the current step and, if possible, suggest which step it might be related to or what action the user appears to be performing instead.
    3. Based on your analysis, determine the overall status using one of the following codes:
       - PROCESSING: At least one checkpoint is in progress or completed, but not all are completed.
       - COMPLETED: All checkpoints are completed.
       - NOT_STARTED: No checkpoints have been started.
       - UNRELATED: The current image reflects a different step or an unrelated action.

    Ensure that your analysis for each checkpoint and the overall step is thorough and based on specific visual evidence from the image.

GPTGuidancePipeline:
  TASK_DESCRIPTION: |
    Task 1: Arrange Flowers
    Goal: Trim the flowers and put them into a vase filled with nutrition water.
    Tools Needed:
    - Vase
    - Flower food
    - Scissors
    - 16oz of water
    Steps:
    1. Prepare the Vase:
       - Open 1/2 bag of flower food.
       - Pour it into the vase. \n
    2. Add Water:
       - Pour 16oz of water into the vase. \n
    3. Trim Leaves:
       - Use scissors to cut the leaves below the waterline. \n
    4. Trim Stems:
       - Use scissors to cut 2-3 inches off the stems.
       - Immediately place the flowers in the vase. \n
    5. Final Placement:
       - Ensure the flowers are arranged neatly.
       - Make sure the flowers are fully submerged in the water. \n

    Task 2: Make Coffee
    Goal: Make a cup of coffee using a coffee filter, ground beans, and water.
    Tools Needed:
    - Coffee beans (11g)
    - Scale
    - Grinder
    - Cone-like coffee filter
    - Brewer
    - Cup
    - Water
    - Timer
    Steps:
    1. Measure and Grind Beans:
       - Measure 11g of coffee beans with a scale.
       - Put the beans into the grinder.
       - Grind until they become coffee powder. \n
    2. Prepare the Filter:
       - Open a cone-like coffee filter.
       - Place it in the brewer. \n
    3. Add Coffee Powder:
       - Place the coffee powder into the filter.
       - Put the cup and brewer on the scale.
       - Set the scale to zero. \n
    4. Pour water:
       - Start a timer.
       - Pour 70g water in a circular motion over the coffee grounds.
       - Wait until the timer reaches 30 seconds. \n
    5. Serve Coffee:
       - Discard the coffee filter into the trash.
       - Remove the brewer.
       - Serve the coffee. \n

    Task 3: Clean Room
    Goal: Assemble a Swiffer mop and duster to clean the floor and desk.
    Tools Needed:
    - Swiffer mop
    - Mop pads
    - Duster
    - Duster handle
    Steps:
    1. Assemble the Mop:
       - Hold the mop poles.
       - Fasten the button at each section of the pole to securely connect them.
       - Attach the poles to the mop head.
       - Take out a mop pad.
       - Wrap the pad around the mop head.
       - Secure the pad into the Swiffer mop head slots. \n
    2. Assemble the Duster:
       - Line up the grooves on the duster and handle.
       - Connect the handle by sliding them together.
       - Fluff the duster to make it effective for dusting.
       - Slide the handle into the duster all the way to the top to secure it. \n
    3. Mop the Floor:
       - Sweep the mop across the floor to pick up dirt, dust, and debris.
       - Ensure all areas are covered by moving the mop back and forth.
       - Periodically check the mop pad and replace it if it becomes too dirty. \n
    4. Dust the Desk:
       - Hold the duster by the handle.
       - Sweep the duster across the desk surface to pick up dust.
       - Make sure to cover all areas, including corners and edges.
       - Fluff the duster as needed to maintain effectiveness. \n

    Task 4: Connect Switch
    Tools Needed:
    - HDMI cable
    - Power cable
    - Monitor
    - Switch dock
    Steps:
    1. Identify Ports:
       - Examine the switch to find the correct ports for the HDMI cable and power cable. \n
    2. Connect HDMI Cable:
       - Plug one end of the HDMI cable into the monitor.
       - Plug the other end into the switch dock. \n
    3. Connect Power Cable:
       - Connect the power cable to the switch dock.
       - Plug the other end into an AC outlet to supply power. \n
    5. Insert Switch:
       - Insert the switch into the dock, ensuring it fits securely. \n
    6. Power On Monitor:
       - Press the power button on the monitor to turn it on. \n
    7. Power On Switch:
       - Press the power button on the switch to turn it on and start displaying on the monitor. \n

  PROMPT_TEXT: |
    You are an AR assistant helping the user with tasks. You will be given a full task description <TASK_DESCRIPTION>, the next step the user will do <NEXT_STEP> and the expertise of user <EXPERTISE>. <TASK_DESCRIPTION> is a list of possible tasks and the user might conduct one of the tasks. Your goal to generate contextual and useful assistance based on the given query. 
    Please output the following information in the response:

    1. <DESIRE> [Based on the given <NEXT_STEP>, generate the user's high-level desire. Refer to the <TASK_DESCRIPTION> for possible tasks' goal. Output this high-level desire prefixed with <DESIRE>.]
    2. <INTENT> [Describe a basic, concrete action in the step]
    3. <META_INTENT> [Generate meta-intent from given meta-intent list [make a tool, interact with time-dependent tools, interact with time-independent tools, interact with materials] based on the user_intent <INTENT>. Output this single meta-intent prefixed with <META_INTENT>.   The meta-intent refers the user's most fundamental intent without the contextual information.   make a tool example intent: assemble Swiffer mob, make coffee filter, arrange flower creatively;   interact with time-dependent tool example intent: use grinder to grind coffee, heat food using microwave;   interact with time-independent example intent: connect to VR headset, use a mop, use a stainer;   interact with materials example intent: add ingredients to a bowl, pour water into a cup, cut flower stems]
    4. <GUIDANCE_TYPE> [Based on the identified <META_INTENT>, select the corresponding guidance type from the following mappings:    {"make a tool": "image",   "interact with time dependent-tools": "timer",   "interact with time independent-tools": "image",   "interact with materials": "note"}]
    5. <OBJECT_LIST> [Identify key objects which the user is interacting with following the <STEP_DESCRIPTION> and the properties of the objects, e.g. color, shape, texture, size. Output an object interaction list with descriptions of properties.]
    6. <TEXT_GUIDANCE_CONTENT> [Generate text guidance content best fit for the user in this step. This guidance should consider the contextual information, e.g. the properties object in real environment, the tips that user should pay attention to. Output based on  the <STEP_DESCRIPTION> and user's <INTENT>, the object interaction list <OBJECT_LIST> and level of detail <LOD>,starting with <TEXT_GUIDANCE_TITLE> and <TEXT_GUIDANCE_CONTENT>. NOT generate text guidance for <DESIRE>, only generate guidance for the fundamental action <INTENT>   In <TEXT_GUIDANCE_CONTENT>, incorporate concrete numbers as required by the <TASK_DESCRIPTON> if possible.]
    7. <TEXT_GUIDANCE_TITLE> [Short title]
    8. <DALLE_PROMPT> [Based on <INTENT>, <OBJECT_LIST> and <EXPERTISE>, generate a DALLE prompt with the following template in appropriate detail. The prompt should not consider <DESIRE>. The prompt should integrate the intent <INTENT>, assistance <TEXT_GUIDANCE_CONTENT> and object interactions <OBJECT_LIST> to depict action clearly and include a red arrow (<INDICATOR>) showing action direction.    if <EXPERTISE> is novice, prompt DALLE to show actions and interacting objects using the template: "<INTENT>or<TEXT_GUIDANCE_CONTENT> <OBJECT_LIST>. <INDICATOR>".    if <EXPERTISE> is expert, prompt DALLE to show final result of the <INTENT> using the template: "<INTENT>or<TEXT_GUIDANCE_CONTENT> <OBJECT_LIST>.<INDICATOR>".]
    9. <HIGHLIGHT_OBJECT_FLAG> [True if key objects in the step to highlight]
    10. <HIGHLIGHT_OBJECT_LOC> [Location of key object if applicable]
    11. <HIGHLIGHT_OBJECT_LABEL> [Name of key object if applicable]
    12. <CONFIRMATION_CONTENT> [Select confirmation content and insert <INTENT><GUIDANCE_TYPE> into sentence, starting with <CONFIRMATION_CONTENT>: "Looks like you are going to <INTENT>, do you need <GUIDANCE_TYPE>?"]

GPTGuidancePipeline:
  PROMPT_TEXT_ALWAYS_v1: |
    You are an AR assistant helping the user with tasks. You will be given a full task description <TASK_DESCRIPTION>, the next step the user will do <NEXT_STEP> and the expertise of user <EXPERTISE>. <TASK_DESCRIPTION> is a list of possible tasks and the user might conduct one of the tasks. Your goal is to generate contextual and useful assistance based on the given query. 
    Please output the following information in the response:

    1. <DESIRE> [Based on the given <NEXT_STEP>, generate the user's high-level desire. Refer to the <TASK_DESCRIPTION> for possible tasks' goal. Output this high-level desire prefixed with <DESIRE>.]
    2. <INTENT> [Describe a basic, concrete action in the step]
    3. <META_INTENT> [Generate meta-intent from given meta-intent list [make a tool, interact with time-dependent tools, interact with time-independent tools, interact with materials] based on the user_intent <INTENT>. Output this single meta-intent prefixed with <META_INTENT>.    The meta-intent refers the user's most fundamental intent without the contextual information.    make a tool example intent: assemble Swiffer mob, make coffee filter, arrange flower creatively;    interact with time-dependent tool example intent: use grinder to grind coffee, heat food using microwave;    interact with time-independent example intent: connect to VR headset, use a mop, use a stainer;    interact with materials example intent: add ingredients to a bowl, pour water into a cup, cut flower stems]
    4. <GUIDANCE_TYPE> [Based on the identified <META_INTENT>, select the corresponding guidance type from the following mappings:    {"make a tool": "image",   "interact with time dependent-tools": "timer",   "interact with time independent-tools": "image",   "interact with materials": "image"}]
    5. <OBJECT_LIST> [Identify key objects which the user is interacting with following the <STEP_DESCRIPTION> and the properties of the objects, e.g. color, shape, texture, size. Output an object interaction list with descriptions of properties.]
    6. <TEXT_GUIDANCE_TITLE> [Short title for text guidance]
    7. <TEXT_GUIDANCE_CONTENT> [Short title in 8 words]
    8. <DALLE_PROMPT> [Based on <INTENT>, <OBJECT_LIST> and <EXPERTISE>, generate a DALLE prompt with the following template in appropriate detail. The prompt should not consider <DESIRE>. The prompt should integrate the intent <INTENT>, assistance <TEXT_GUIDANCE_CONTENT> and object interactions <OBJECT_LIST> to depict action clearly and include a red arrow (<INDICATOR>) showing action direction.    if <EXPERTISE> is novice, prompt DALLE to show actions and interacting objects using the template: "<INTENT>or<TEXT_GUIDANCE_CONTENT> <OBJECT_LIST>. <INDICATOR>".    if <EXPERTISE> is expert, prompt DALLE to show final result of the <INTENT> using the template: "<INTENT>or<TEXT_GUIDANCE_CONTENT> <OBJECT_LIST>.<INDICATOR>".]
    9. <HIGHLIGHT_OBJECT_FLAG> [True if key objects to highlight]
    10. <HIGHLIGHT_OBJECT_LOC> [Location of key object if applicable]
    11. <HIGHLIGHT_OBJECT_LABEL> [Name of key object if applicable]
    12. <CONFIRMATION_CONTENT> [Select confirmation content based on <META_INTENT> from the following options, and insert <INTENT> into sentence, starting with <CONFIRMATION_CONTENT>: "Looks like you are going to <INTENT>, do you need <GUIDANCE_TYPE>?"]

  PROMPT_TEXT_ALWAYS_v2: |
    You are an AR assistant helping users with tasks. Given an image, a task guidance and the next step, generate guidance in the required format. Please make sure your guidance is not too simple and can actually help the user.

    <TASK_DESCRIPTION>: [Task description]
    <NEXT_STEP>: [Description of the next step]
    <image>: [first-person perspective image of the user's environment]

    <INSTRUCTIONS>:
    Based on the <NEXT_STEP>, provide the following:
    0.<DESORE> [Based on the given <NEXT_STEP>, generate the user's high-level goal. Refer to the <TASK_DESCRIPTION> for possible tasks' goal. Output this high-level desire prefixed with <DESIRE>.]
    1. <INTENT> [Describe a basic, concrete action in the step. keep concise and clear]
    2. <META_INTENT> [Generate meta-intent from given meta-intent list [make a tool, interact with time-dependent tools, interact with time-independent tools, interact with materials] based on the user_intent <INTENT>. Output this single meta-intent prefixed with <META_INTENT>.    The meta-intent refers the user's most fundamental intent without the contextual information.    make a tool example intent: assemble Swiffer mob, make coffee filter, arrange flower creatively;    interact with time-dependent tool example intent: use grinder to grind coffee, heat food using microwave;    interact with time-independent example intent: connect to VR headset, use a mop, use a stainer;    interact with materials example intent: add ingredients to a bowl, pour water into a cup, cut flower stems]
    3. <GUIDANCE_TYPE> [Select between 'image' and 'timer'. Based on the identified <META_INTENT>, select the corresponding guidance type from the following mappings:    {"make a tool": "image",   "interact with time dependent-tools": "timer",   "interact with time independent-tools": "image",   "interact with materials": "image"}]
    4. <TEXT_GUIDANCE_TITLE> [Short title]
    5. <TEXT_GUIDANCE_CONTENT> [Short title in 8 words]
    6. <DALLE_PROMPT> [Based on <INTENT>, <OBJECT_LIST> and <EXPERTISE>, generate a DALLE prompt with the following template in appropriate detail. The prompt should not consider <DESIRE>. The prompt should integrate the intent <INTENT>, assistance <TEXT_GUIDANCE_CONTENT> and object interactions <OBJECT_LIST> to depict action clearly and include a red arrow (<INDICATOR>) showing action direction.    if <EXPERTISE> is novice, prompt DALLE to show actions and interacting objects using the template: "<INTENT>or<TEXT_GUIDANCE_CONTENT> <OBJECT_LIST>. <INDICATOR>".    if <EXPERTISE> is expert, prompt DALLE to show final result of the <INTENT> using the template: "<INTENT>or<TEXT_GUIDANCE_CONTENT> <OBJECT_LIST>.<INDICATOR>".]
    7. <OBJECT_LIST> [Key objects with properties in the image: identify key objects which the user is interacting with following the <STEP_DESCRIPTION> and the properties of the objects, e.g. color, shape, texture, size. Output an object interaction list with descriptions of properties.]
    8. <HIGHLIGHT_OBJECT_FLAG> [True if key objects to highlight]
    9. <HIGHLIGHT_OBJECT_LOC> [Location of key object if applicable]
    10. <HIGHLIGHT_OBJECT_LABEL> [Name of key object if applicable]
    11. <CONFIRMATION_CONTENT> [Select confirmation content based on <META_INTENT> from the following options, and insert <INTENT> into sentence, starting with <CONFIRMATION_CONTENT>: "Looks like you are going to <INTENT>, do you need <GUIDANCE_TYPE>?"]

SAMPLE_INPUT: |
    ""
    1. Measure and Grind Beans:
       - Measure 11g of coffee beans with a scale.
       - Put the beans into the grinder.
       - Grind until they become coffee powder. \n
    2. Prepare the Filter:
       - Open a cone-like coffee filter.
       - Place it in the brewer. \n
    3. Wet the Filter:
       - Place the brewer on the cup.
       - Pour enough water into the filter to wet it.
       - Remove any papery residue. \n
    4. Discard Water:
       - Lift the brewer.
       - Pour out the water.
       - Set the brewer with the wet filter back on the cup. \n
    5. Add Coffee Powder:
       - Place the coffee powder into the filter.
       - Put the cup and brewer on the scale.
       - Set the scale to zero. \n
    6. First Pour:
       - Start a timer.
       - Pour water in a circular motion over the coffee grounds until the scale shows 50g.
       - Wait until the timer reaches 30 seconds. \n
    7. Second Pour:
       - Start a new timer.
       - Pour water slowly into the coffee until it reaches 140g.
       - Wait until the timer reaches 30 seconds. \n
    ""

TaskPlannerPipeline:
  SYSTEM_PROMPT: |
      You are an intelligent task planner. Your job is to break down a given task into structured steps and checkpoints that an AR assistant can use for guiding a user. Each step must have clear instructions and checkpoints, with each checkpoint designed to verify the completion of a sub-task. For each checkpoint, generate a specific question to be used by a vision-language model (such as BLIP) to confirm if the checkpoint is completed.

      Additionally, you need to provide a list of "StepStatusCheckPrompts" for each step. These prompts should help the BLIP model verify whether the user is currently in the correct step. Consider that the BLIP model can only access static visual information, so the prompts should be carefully written to focus on visual cues and context. If at least 50% of these checks succeed, the system will infer that the user is in the correct step.

      Also, list all the objects involved in the task.

      Output the task plan in XML format with the following structure. Please provide the output in plain text without any code formatting or language hints:

      
      <TaskPlan>
          <DesiredTask>Your task name here</DesiredTask>
          <Steps>
              <Step isStart="true/false">
                  <StepContent>
                      Measure 11 grams of coffee beans, grind them into powder using a grinder
                  </StepContent>
                  <StepStatusCheckPrompts>
                      <Prompt>First prompt to check if the user is currently in this step</Prompt>
                      <Prompt>Second prompt to check if the user is currently in this step</Prompt>
                      <!-- Repeat for additional prompts -->
                  </StepStatusCheckPrompts>
                  <Checkpoints>
                      <Checkpoint>
                          <Instruction>Step instruction here</Instruction>
                          <BlipPrompt>Prompt for BLIP model here</BlipPrompt>
                      </Checkpoint>
                      <!-- Repeat for additional checkpoints -->
                  </Checkpoints>
              </Step>
              <!-- Repeat for additional steps -->
          </Steps>
          <Objects>
              <Object>Object name here</Object>
              <!-- Repeat for additional objects -->
          </Objects>
      </TaskPlan>
